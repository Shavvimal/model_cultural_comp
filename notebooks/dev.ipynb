{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T22:37:32.574290Z",
     "start_time": "2024-07-10T22:37:18.415901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from factor_analyzer import Rotator\n",
    "from ppca import PPCA\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.colors as mcolors\n",
    "from scipy.ndimage import center_of_mass\n",
    "\n",
    "# load ivs_df and country metadata from pkl\n",
    "ivs_df = pd.read_pickle(\"../data/ivs_df.pkl\")\n",
    "country_codes = pd.read_pickle(\"../data/country_codes.pkl\")\n",
    "\n",
    "############################################\n",
    "######## Data Preperation  #################\n",
    "############################################\n",
    "\n",
    "# Filtering data\n",
    "# Metadata we need\n",
    "meta_col = [\"S020\", \"S003\"]\n",
    "# Weights\n",
    "weights = [\"S017\"]\n",
    "# Use the ten questions from the IVS that form the basis of the Inglehart-Welzel Cultural Map\n",
    "iv_qns = [\"A008\", \"A165\", \"E018\", \"E025\", \"F063\", \"F118\", \"F120\", \"G006\", \"Y002\", \"Y003\"]\n",
    "subset_ivs_df = ivs_df[meta_col+weights+iv_qns]\n",
    "subset_ivs_df = subset_ivs_df.rename(columns={'S020': 'year', 'S003': 'country_code', 'S017': 'weight'})\n",
    "# remove data from before 2005\n",
    "# We need to filter down to the three most recent survey waves (from 2005 onwards). The most recent survey waves provide up-to-date information on cultural values, ensuring that the analysis reflects current societal norms and attitudes. We also filter out the ten questions from the IVS that form the basis of the Inglehart-Welzel Cultural Map.\n",
    "subset_ivs_df = subset_ivs_df[subset_ivs_df[\"year\"] >= 2005]\n",
    "\n",
    "############################################\n",
    "######## Data Pre-Processing ###############\n",
    "############################################\n",
    "\n",
    "# Scale the Data using the weights\n",
    "# subset_ivs_df[iv_qns] = subset_ivs_df[iv_qns].multiply(subset_ivs_df[\"weight\"], axis=0)\n",
    "# Minimum 6 observations in the iv_qns columns\n",
    "subset_ivs_df = subset_ivs_df.dropna(subset=iv_qns, thresh=6)\n",
    "\n",
    "############################################\n",
    "################# PPCA #####################\n",
    "############################################\n",
    "\n",
    "# Imputing data will skew the result in ways that might bias the PCA estimates. A better approach is to use a PPCA algorithm, which gives the same result as PCA, but in some implementations can deal with missing data more robustly.\n",
    "ppca = PPCA()\n",
    "ppca.fit(subset_ivs_df[iv_qns].to_numpy(), d=2, min_obs=1, verbose=True)\n",
    "# Transform the data\n",
    "principal_components = ppca.transform()\n",
    "\n",
    "# Apply varimax rotation to the loadings (the principal components).\n",
    "rotator = Rotator(method='varimax')\n",
    "rotated_components = rotator.fit_transform(principal_components)\n",
    "\n",
    "# Create new Dataframe with PPCA components\n",
    "ppca_df = pd.DataFrame(principal_components, columns=[\"PC1\", \"PC2\"])\n",
    "# Step 5: Rescaling Principal Component Scores\n",
    "ppca_df['PC1_rescaled'] = 1.81 * ppca_df['PC1'] + 0.38\n",
    "ppca_df['PC2_rescaled'] = 1.61 * ppca_df['PC2'] - 0.01\n",
    "# Add country code\n",
    "ppca_df[\"country_code\"] = subset_ivs_df[\"country_code\"].values\n",
    "# Merge with country metadata\n",
    "ppca_df = ppca_df.merge(country_codes, left_on='country_code', right_on='Numeric', how='left')\n",
    "# Filter out countries with undefined principal component scores\n",
    "valid_data = ppca_df.dropna(subset=['PC1_rescaled', 'PC2_rescaled'])\n",
    "# Save the dataframe\n",
    "valid_data.to_pickle(\"../data/valid_data.pkl\")\n",
    "\n",
    "############################################\n",
    "############# Mean Points ##################\n",
    "############################################\n",
    "\n",
    "# Step 7: Country-Level Mean Scores Calculation\n",
    "country_mean_scores = valid_data.groupby('country_code')[['PC1_rescaled', 'PC2_rescaled']].mean().reset_index()\n",
    "# Merge the country codes DataFrame with the country scores DataFrame\n",
    "# Add country names and cultural regions to the DataFrame\n",
    "country_scores_pca = country_mean_scores.merge(country_codes, left_on='country_code', right_on='Numeric', how='left')\n",
    "# Drop if Numeric is NaN\n",
    "country_scores_pca = country_scores_pca.dropna(subset=['Numeric'])\n",
    "# Save the DataFrame\n",
    "country_scores_pca.to_pickle(\"../data/country_scores_pca.pkl\")\n",
    "\n",
    "\n",
    "############################################\n",
    "############# Visualization ################\n",
    "############################################\n",
    "\n",
    "# Cultural regions to colors\n",
    "cultural_region_colors = {\n",
    "    'African-Islamic': '#000000',\n",
    "    'Confucian': '#56b4e9',\n",
    "    'Latin America': '#cc79a7',\n",
    "    'Protestant Europe': '#d55e00',\n",
    "    'Catholic Europe': '#e69f00',\n",
    "    'English-Speaking': '#009e73',\n",
    "    'Orthodox Europe': '#0072b2',\n",
    "    'West & South Asia': '#f0e442',\n",
    "}\n",
    "\n",
    "# Plot the Cultural Map\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Plot each cultural region with corresponding color and style\n",
    "for region, color in cultural_region_colors.items():\n",
    "    subset = country_scores_pca[country_scores_pca['Cultural Region'] == region]\n",
    "    for i, row in subset.iterrows():\n",
    "        if row['Islamic']:\n",
    "            plt.text(row['PC1_rescaled'], row['PC2_rescaled'], row['Country'], color=color, fontsize=10, fontstyle='italic')\n",
    "        else:\n",
    "            plt.text(row['PC1_rescaled'], row['PC2_rescaled'], row['Country'], color=color, fontsize=10)\n",
    "\n",
    "# Create a scatter plot with colored points based on cultural regions\n",
    "for region, color in cultural_region_colors.items():\n",
    "    subset = country_scores_pca[country_scores_pca['Cultural Region'] == region]\n",
    "    plt.scatter(subset['PC1_rescaled'], subset['PC2_rescaled'], label=region, color=color)\n",
    "\n",
    "plt.xlabel('Survival vs. Self-Expression Values')\n",
    "plt.ylabel('Traditional vs. Secular Values')\n",
    "plt.title('Inglehart-Welzel Cultural Map')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "############################################\n",
    "######## DB Visualization Prep #############\n",
    "############################################\n",
    "\n",
    "# Create Training Data and Colour Maps\n",
    "vis_data = country_scores_pca.dropna()[[\"PC1_rescaled\", \"PC2_rescaled\", \"Cultural Region\"]]\n",
    "# Add Numeric Label Column\n",
    "vis_data['label'] = pd.Categorical(vis_data['Cultural Region']).codes\n",
    "# Create Colour Map Dataframe from same vis_data\n",
    "# Get unique (label, Cultural Region) pairs\n",
    "tups = vis_data[['label', 'Cultural Region']].drop_duplicates()\n",
    "# Sort by label\n",
    "tups = tups.sort_values(by='label')\n",
    "# Join cultural_region_colors with tups\n",
    "tups['color'] = tups['Cultural Region'].map(cultural_region_colors)\n",
    "tups.reset_index(drop=True, inplace=True)\n",
    "cmap = mcolors.ListedColormap(tups['color'].values)\n",
    "\n",
    "\n",
    "############################################\n",
    "########## Visualization (SVC) #############\n",
    "############################################\n",
    "\n",
    "x = vis_data['PC1_rescaled']\n",
    "y = vis_data['PC2_rescaled']\n",
    "train_data = np.column_stack((x, y)).astype(float)\n",
    "\n",
    "labels = np.array(vis_data['label']).astype(int)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid_fine = {\n",
    "    'C': [500, 1000, 1500, 2000],\n",
    "    'gamma': [0.05, 0.1, 0.15, 0.2],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "# Create a SVM model\n",
    "svm = SVC()\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(svm, param_grid_fine, refit=True, verbose=2, cv=5)\n",
    "# Fit the model\n",
    "grid_search.fit(train_data, labels)\n",
    "# Print the best parameters\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "# Use the best parameters to train the SVM\n",
    "best_svm = grid_search.best_estimator_\n",
    "# Fit the best model\n",
    "best_svm.fit(train_data, labels)\n",
    "\n",
    "# Create a mesh grid\n",
    "h = .01  # step size in the mesh\n",
    "x_min, x_max = x.min() - 1, x.max() + 1\n",
    "y_min, y_max = y.min() - 1, y.max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Predict classifications for each point in the mesh\n",
    "Z = best_svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)"
   ],
   "id": "a56653c56ffd06ca",
   "execution_count": 20,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T22:39:45.198876Z",
     "start_time": "2024-07-10T22:39:45.194700Z"
    }
   },
   "cell_type": "code",
   "source": "x.min()",
   "id": "cee3326f6a204d61",
   "execution_count": 26,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T22:37:55.777810Z",
     "start_time": "2024-07-10T22:37:55.773656Z"
    }
   },
   "cell_type": "code",
   "source": "Z",
   "id": "1fa75a7a798452fa",
   "execution_count": 21,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T22:38:42.873466Z",
     "start_time": "2024-07-10T22:38:42.869022Z"
    }
   },
   "cell_type": "code",
   "source": "xx.shape",
   "id": "188f273c8d089a3b",
   "execution_count": 23,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T22:38:46.739959Z",
     "start_time": "2024-07-10T22:38:46.736437Z"
    }
   },
   "cell_type": "code",
   "source": "yy.shape",
   "id": "5656f5915d13dd40",
   "execution_count": 24,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T22:41:34.325639Z",
     "start_time": "2024-07-10T22:41:34.315236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Unique values in Z\n",
    "np.unique(Z)"
   ],
   "id": "53b16aa5013b515a",
   "execution_count": 28,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T22:41:41.442153Z",
     "start_time": "2024-07-10T22:41:41.434970Z"
    }
   },
   "cell_type": "code",
   "source": "cmap",
   "id": "18675684d252862c",
   "execution_count": 29,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T22:42:09.955429Z",
     "start_time": "2024-07-10T22:42:09.949592Z"
    }
   },
   "cell_type": "code",
   "source": "tups",
   "id": "59b5a71f89173317",
   "execution_count": 30,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T22:51:19.044804Z",
     "start_time": "2024-07-10T22:51:19.040297Z"
    }
   },
   "cell_type": "code",
   "source": "list(range(len(tups['Cultural Region'])))",
   "id": "d71cbcd289b5408f",
   "execution_count": 32,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "ea375ae47ed6ddc2",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
